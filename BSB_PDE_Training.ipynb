{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from networks import *\n",
    "from fbsde import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h = 1\n",
    "epsilon = 0.01\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_per_dim = 5\n",
    "batch_size = 100\n",
    "num_iterations = 66000\n",
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "runs = 10\n",
    "r = 0.05\n",
    "volatility = 0.4\n",
    "T = 1\n",
    "fbsde = BS_Barenblatt(volatility, r, 1)\n",
    "#optimizer = \"LBFGS\" # Can use LBFGS or Adam\n",
    "#learning_rate = 1\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_diff(pde, u, t, x):\n",
    "  u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "  Du = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "  #Hessian H[i][j] is derivative with respect to jth variable then with respect to ith variable or is it the other way around\n",
    "  I_N = torch.eye(x.shape[-1], device=device)\n",
    "  def get_vjp(v):\n",
    "    return torch.autograd.grad(Du, x, grad_outputs=v.repeat(x.shape[0], 1), create_graph=True)\n",
    "  D2u = torch.vmap(get_vjp)(I_N)[0]\n",
    "  if len(x.shape) > 1:\n",
    "    D2u = D2u.swapaxes(0, 1)\n",
    "  A = D2u @ pde.sigma(t,x,u) @ pde.sigma(t, x, u).transpose(-2, -1)\n",
    "  trace = torch.diagonal(A, dim1=-2, dim2=-1).sum(dim=-1, keepdim=True)\n",
    "  #trace = torch.vmap(torch.trace)(A)\n",
    "  # in the code D2u[sample][i][j] is the derivative with respect to ith variable then jth variable\n",
    "  f = pde.phi(t, x, u, Du) - torch.sum(Du * pde.mu(t, x, u, Du), dim=-1, keepdim=True) - 1/2 * trace\n",
    "  return mse(u_t, f)\n",
    "\n",
    "def loss_bc(pde, u, x):\n",
    "  return mse(pde.g(x), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of interior training points is specific to the Black-Scholes-Barenblatt PDE as there is a simple closed form solution for the forward process in the associated PDE and so we can train on points chosen in a similar way to the FBSDE loss case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 5.548e+03\n",
      "Iteration: 1000, Loss: 5.736e+01\n",
      "Iteration: 2000, Loss: 4.259e+01\n",
      "Iteration: 3000, Loss: 3.520e+01\n",
      "Iteration: 4000, Loss: 3.900e+01\n",
      "Iteration: 5000, Loss: 5.461e+01\n",
      "Iteration: 6000, Loss: 2.995e+01\n",
      "Iteration: 7000, Loss: 6.977e+01\n",
      "Iteration: 8000, Loss: 2.105e+01\n",
      "Iteration: 9000, Loss: 2.933e+01\n",
      "Iteration: 10000, Loss: 2.945e+01\n",
      "Iteration: 11000, Loss: 3.522e+01\n",
      "Iteration: 12000, Loss: 2.073e+01\n",
      "Iteration: 13000, Loss: 2.818e+01\n",
      "Iteration: 14000, Loss: 1.924e+01\n",
      "Iteration: 15000, Loss: 2.404e+01\n",
      "Iteration: 16000, Loss: 4.114e+01\n",
      "Iteration: 17000, Loss: 2.426e+01\n",
      "Iteration: 18000, Loss: 2.158e+01\n",
      "Iteration: 19000, Loss: 3.909e+01\n",
      "Iteration: 20000, Loss: 5.680e+01\n",
      "Iteration: 21000, Loss: 2.824e+01\n",
      "Iteration: 22000, Loss: 3.592e+01\n",
      "Iteration: 23000, Loss: 2.802e+01\n",
      "Iteration: 24000, Loss: 1.922e+01\n",
      "Iteration: 25000, Loss: 1.850e+01\n",
      "Iteration: 26000, Loss: 4.137e+01\n",
      "Iteration: 27000, Loss: 2.572e+01\n",
      "Iteration: 28000, Loss: 4.837e+01\n",
      "Iteration: 29000, Loss: 2.268e+01\n",
      "Iteration: 30000, Loss: 2.254e+01\n",
      "Iteration: 31000, Loss: 3.606e+01\n",
      "Iteration: 32000, Loss: 3.243e+01\n",
      "Iteration: 33000, Loss: 2.769e+01\n",
      "Iteration: 34000, Loss: 3.085e+01\n",
      "Iteration: 35000, Loss: 3.365e+01\n",
      "Iteration: 36000, Loss: 2.412e+01\n",
      "Iteration: 37000, Loss: 3.479e+01\n",
      "Iteration: 38000, Loss: 3.481e+01\n",
      "Iteration: 39000, Loss: 2.915e+01\n",
      "Iteration: 40000, Loss: 7.772e+01\n",
      "Iteration: 41000, Loss: 1.893e+01\n",
      "Iteration: 42000, Loss: 2.434e+01\n",
      "Iteration: 43000, Loss: 1.826e+01\n",
      "Iteration: 44000, Loss: 1.797e+01\n",
      "Iteration: 45000, Loss: 4.702e+01\n",
      "Iteration: 46000, Loss: 2.203e+01\n",
      "Iteration: 47000, Loss: 3.851e+01\n",
      "Iteration: 48000, Loss: 3.543e+01\n",
      "Iteration: 49000, Loss: 2.434e+01\n",
      "Iteration: 50000, Loss: 2.212e+01\n",
      "Iteration: 51000, Loss: 2.649e+01\n",
      "Iteration: 52000, Loss: 2.810e+01\n",
      "Iteration: 53000, Loss: 2.736e+01\n",
      "Iteration: 54000, Loss: 2.591e+01\n",
      "Iteration: 55000, Loss: 2.361e+01\n",
      "Iteration: 56000, Loss: 2.814e+01\n",
      "Iteration: 57000, Loss: 3.102e+01\n",
      "Iteration: 58000, Loss: 4.444e+01\n",
      "Iteration: 59000, Loss: 3.169e+01\n",
      "Iteration: 60000, Loss: 2.282e+01\n",
      "Iteration: 61000, Loss: 1.625e+01\n",
      "Iteration: 62000, Loss: 1.562e+01\n",
      "Iteration: 63000, Loss: 2.594e+01\n",
      "Iteration: 64000, Loss: 2.370e+01\n",
      "Iteration: 65000, Loss: 2.880e+01\n",
      "Iteration: 0, Loss: 5.686e+03\n",
      "Iteration: 1000, Loss: 5.021e+01\n",
      "Iteration: 2000, Loss: 5.738e+01\n",
      "Iteration: 3000, Loss: 3.590e+01\n",
      "Iteration: 4000, Loss: 1.950e+01\n",
      "Iteration: 5000, Loss: 3.274e+01\n",
      "Iteration: 6000, Loss: 3.417e+01\n",
      "Iteration: 7000, Loss: 2.717e+01\n",
      "Iteration: 8000, Loss: 1.789e+01\n",
      "Iteration: 9000, Loss: 4.086e+01\n",
      "Iteration: 10000, Loss: 2.952e+01\n",
      "Iteration: 11000, Loss: 2.825e+01\n",
      "Iteration: 12000, Loss: 2.561e+01\n",
      "Iteration: 13000, Loss: 1.184e+02\n",
      "Iteration: 14000, Loss: 2.876e+01\n",
      "Iteration: 15000, Loss: 2.137e+01\n",
      "Iteration: 16000, Loss: 2.284e+01\n",
      "Iteration: 17000, Loss: 2.878e+01\n",
      "Iteration: 18000, Loss: 2.286e+01\n",
      "Iteration: 19000, Loss: 8.074e+01\n",
      "Iteration: 20000, Loss: 3.043e+01\n",
      "Iteration: 21000, Loss: 2.168e+01\n",
      "Iteration: 22000, Loss: 2.526e+01\n",
      "Iteration: 23000, Loss: 2.867e+01\n",
      "Iteration: 24000, Loss: 3.794e+01\n",
      "Iteration: 25000, Loss: 2.718e+01\n",
      "Iteration: 26000, Loss: 2.838e+01\n",
      "Iteration: 27000, Loss: 3.956e+01\n",
      "Iteration: 28000, Loss: 2.104e+01\n",
      "Iteration: 29000, Loss: 2.927e+01\n",
      "Iteration: 30000, Loss: 3.601e+01\n",
      "Iteration: 31000, Loss: 2.149e+01\n",
      "Iteration: 32000, Loss: 2.875e+01\n",
      "Iteration: 33000, Loss: 2.788e+01\n",
      "Iteration: 34000, Loss: 2.565e+01\n",
      "Iteration: 35000, Loss: 2.759e+01\n",
      "Iteration: 36000, Loss: 3.225e+01\n",
      "Iteration: 37000, Loss: 1.753e+01\n",
      "Iteration: 38000, Loss: 4.580e+01\n",
      "Iteration: 39000, Loss: 2.766e+01\n",
      "Iteration: 40000, Loss: 3.658e+01\n",
      "Iteration: 41000, Loss: 1.986e+01\n",
      "Iteration: 42000, Loss: 1.969e+01\n",
      "Iteration: 43000, Loss: 2.580e+01\n",
      "Iteration: 44000, Loss: 2.359e+01\n",
      "Iteration: 45000, Loss: 1.584e+01\n",
      "Iteration: 46000, Loss: 2.905e+01\n",
      "Iteration: 47000, Loss: 3.626e+01\n",
      "Iteration: 48000, Loss: 3.865e+01\n",
      "Iteration: 49000, Loss: 3.556e+01\n",
      "Iteration: 50000, Loss: 2.202e+01\n",
      "Iteration: 51000, Loss: 2.468e+01\n",
      "Iteration: 52000, Loss: 3.132e+01\n",
      "Iteration: 53000, Loss: 1.475e+01\n",
      "Iteration: 54000, Loss: 4.913e+01\n",
      "Iteration: 55000, Loss: 1.912e+01\n",
      "Iteration: 56000, Loss: 1.577e+01\n",
      "Iteration: 57000, Loss: 2.584e+01\n",
      "Iteration: 58000, Loss: 1.975e+01\n",
      "Iteration: 59000, Loss: 6.714e+01\n",
      "Iteration: 60000, Loss: 2.482e+01\n",
      "Iteration: 61000, Loss: 4.553e+01\n",
      "Iteration: 62000, Loss: 1.946e+01\n",
      "Iteration: 63000, Loss: 3.184e+01\n",
      "Iteration: 64000, Loss: 3.898e+01\n",
      "Iteration: 65000, Loss: 3.091e+01\n",
      "Iteration: 0, Loss: 5.637e+03\n",
      "Iteration: 1000, Loss: 4.822e+01\n",
      "Iteration: 2000, Loss: 3.417e+01\n",
      "Iteration: 3000, Loss: 4.998e+01\n",
      "Iteration: 4000, Loss: 4.418e+01\n",
      "Iteration: 5000, Loss: 3.684e+01\n",
      "Iteration: 6000, Loss: 3.471e+01\n",
      "Iteration: 7000, Loss: 3.129e+01\n",
      "Iteration: 8000, Loss: 9.148e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m loss_interior \u001b[38;5;241m=\u001b[39m loss_diff(fbsde, u, interior_ts, interior_xs)\u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     56\u001b[0m boundary_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((boundary_ts, boundary_xs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboundary_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_boundary \u001b[38;5;241m=\u001b[39m (loss_bc(fbsde, u, boundary_xs) \u001b[38;5;241m+\u001b[39m loss_diff(fbsde, u, boundary_ts, boundary_xs))\u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_interior \u001b[38;5;241m+\u001b[39m loss_boundary\n",
      "File \u001b[0;32m~/anaconda3/envs/presentation/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/networks.py:42\u001b[0m, in \u001b[0;36mNAIS_Net_Untied.forward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(u))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_state)):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_forward(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_state[i]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_input[i](u))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for activation in [torch.sin, F.relu]:\n",
    "for activation in [torch.sin]:\n",
    "    activation_str = None\n",
    "    if activation == torch.sin:\n",
    "        activation_str = \"sin\"\n",
    "    elif activation == F.relu:\n",
    "        activation_str = \"relu\"\n",
    "    for d in [100, 3]:\n",
    "        losses = []\n",
    "        boundary_losses = []\n",
    "        times = []\n",
    "        zeta = torch.tensor(int(d / 2) * [1., 0.5] + (d % 2) * [1.], device=device)\n",
    "        for run in range(runs):\n",
    "            run_losses = []\n",
    "            run_boundary_losses = []\n",
    "            run_times = []\n",
    "            pinn_network = NAIS_Net_Untied(d+1, 256, 4, 1, activation, epsilon, h).to(device)\n",
    "            pinn_optimizer = torch.optim.LBFGS(pinn_network.parameters(), lr=learning_rate) if optimizer == \"LBFGS\" else torch.optim.Adam(pinn_network.parameters(), lr=learning_rate)\n",
    "            for iteration in range(num_iterations):\n",
    "                start_time = time.time()\n",
    "                interior_ts = torch.rand((batch_size, 1), requires_grad=True, device=device)\n",
    "                # This is where the markdown remark comes in\n",
    "                interior_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * interior_ts + fbsde.volatility * torch.sqrt(interior_ts) * torch.randn((batch_size, d), device=device))\n",
    "                #interior_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                interior_xs.requires_grad_(True)\n",
    "\n",
    "                boundary_ts = fbsde.T * torch.ones((batch_size, 1), device=device)\n",
    "                boundary_ts.requires_grad_(True)\n",
    "                #boundary_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                boundary_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * boundary_ts + fbsde.volatility * torch.sqrt(boundary_ts) * torch.randn((batch_size, d), device=device))\n",
    "                boundary_xs.requires_grad_(True)\n",
    "\n",
    "                if optimizer == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        pinn_optimizer.zero_grad()\n",
    "                        xs_i = interior_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_i = interior_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_i = torch.cat((ts_i, xs_i), dim=-1)\n",
    "                        u_i = pinn_network(sample_i)\n",
    "                        loss_interior = loss_diff(fbsde, u_i, ts_i, xs_i)/batch_size\n",
    "                        xs_b = boundary_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_b = boundary_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_b = torch.cat((ts_b, xs_b), dim=-1)\n",
    "                        u_b = pinn_network(sample_b)\n",
    "                        loss_boundary = (loss_bc(fbsde, u_b, xs_b) + loss_diff(fbsde, u_b, ts_b, xs_b))/batch_size\n",
    "                        loss = loss_interior + loss_boundary\n",
    "                        loss.backward()\n",
    "                        print(loss)\n",
    "                        return loss\n",
    "                    loss = pinn_optimizer.step(closure)\n",
    "                elif optimizer == \"Adam\":\n",
    "                    pinn_optimizer.zero_grad()\n",
    "                    interior_sample = torch.cat((interior_ts, interior_xs), dim=-1)\n",
    "                    u = pinn_network(interior_sample)\n",
    "                    loss_interior = loss_diff(fbsde, u, interior_ts, interior_xs)/batch_size\n",
    "                    boundary_sample = torch.cat((boundary_ts, boundary_xs), dim=-1)\n",
    "                    u = pinn_network(boundary_sample)\n",
    "                    loss_boundary = (loss_bc(fbsde, u, boundary_xs) + loss_diff(fbsde, u, boundary_ts, boundary_xs))/batch_size\n",
    "                    loss = loss_interior + loss_boundary\n",
    "                    loss.backward()\n",
    "                    pinn_optimizer.step()\n",
    "                    run_boundary_losses.append(loss_boundary.item())\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Iteration: %d, Loss: %.3e\" % (iteration, loss.item()))\n",
    "                    torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_iteration_%d.pt\" % (activation_str, d, run, iteration))\n",
    "                run_losses.append(loss.item())\n",
    "                run_times.append(time.time() - start_time)\n",
    "            losses.append(run_losses)\n",
    "            boundary_losses.append(run_boundary_losses)\n",
    "            times.append(run_times)\n",
    "            torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_trained.pt\" % (activation_str, d, run))\n",
    "        with open(\"PINN_%s/losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(losses, f)\n",
    "        with open(\"PINN_%s/terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(boundary_losses, f)\n",
    "        with open(\"PINN_%s/times_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(run_times, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 5.570e+03\n",
      "Iteration: 1000, Loss: 4.649e+01\n",
      "Iteration: 2000, Loss: 5.260e+01\n",
      "Iteration: 3000, Loss: 3.565e+01\n",
      "Iteration: 4000, Loss: 3.090e+01\n",
      "Iteration: 5000, Loss: 3.792e+01\n",
      "Iteration: 6000, Loss: 3.707e+01\n",
      "Iteration: 7000, Loss: 6.001e+01\n",
      "Iteration: 8000, Loss: 5.097e+01\n",
      "Iteration: 9000, Loss: 3.655e+01\n",
      "Iteration: 10000, Loss: 3.802e+01\n",
      "Iteration: 11000, Loss: 2.956e+01\n",
      "Iteration: 12000, Loss: 1.952e+01\n",
      "Iteration: 13000, Loss: 8.010e+01\n",
      "Iteration: 14000, Loss: 3.027e+01\n",
      "Iteration: 15000, Loss: 3.734e+01\n",
      "Iteration: 16000, Loss: 3.361e+01\n",
      "Iteration: 17000, Loss: 2.671e+01\n",
      "Iteration: 18000, Loss: 3.258e+01\n",
      "Iteration: 19000, Loss: 2.743e+01\n",
      "Iteration: 20000, Loss: 2.676e+01\n",
      "Iteration: 21000, Loss: 4.838e+01\n",
      "Iteration: 22000, Loss: 3.894e+01\n",
      "Iteration: 23000, Loss: 3.975e+01\n",
      "Iteration: 24000, Loss: 2.376e+01\n",
      "Iteration: 25000, Loss: 3.427e+01\n",
      "Iteration: 26000, Loss: 3.448e+01\n",
      "Iteration: 27000, Loss: 9.991e+01\n",
      "Iteration: 28000, Loss: 2.541e+01\n",
      "Iteration: 29000, Loss: 3.567e+01\n",
      "Iteration: 30000, Loss: 2.336e+01\n",
      "Iteration: 31000, Loss: 2.156e+01\n",
      "Iteration: 32000, Loss: 2.592e+01\n",
      "Iteration: 33000, Loss: 3.249e+01\n",
      "Iteration: 34000, Loss: 2.816e+01\n",
      "Iteration: 35000, Loss: 2.870e+01\n",
      "Iteration: 36000, Loss: 2.745e+01\n",
      "Iteration: 37000, Loss: 2.494e+01\n",
      "Iteration: 38000, Loss: 2.735e+01\n",
      "Iteration: 39000, Loss: 3.010e+01\n",
      "Iteration: 40000, Loss: 3.232e+01\n",
      "Iteration: 41000, Loss: 3.374e+01\n",
      "Iteration: 42000, Loss: 3.182e+01\n",
      "Iteration: 43000, Loss: 2.247e+01\n",
      "Iteration: 44000, Loss: 1.608e+02\n",
      "Iteration: 45000, Loss: 1.794e+01\n",
      "Iteration: 46000, Loss: 3.543e+01\n",
      "Iteration: 47000, Loss: 2.569e+01\n",
      "Iteration: 48000, Loss: 2.106e+01\n",
      "Iteration: 49000, Loss: 4.891e+01\n",
      "Iteration: 50000, Loss: 6.040e+01\n",
      "Iteration: 51000, Loss: 2.792e+01\n",
      "Iteration: 52000, Loss: 2.634e+01\n",
      "Iteration: 53000, Loss: 2.401e+01\n",
      "Iteration: 54000, Loss: 2.378e+01\n",
      "Iteration: 55000, Loss: 3.094e+01\n",
      "Iteration: 56000, Loss: nan\n",
      "Iteration: 57000, Loss: nan\n",
      "Iteration: 58000, Loss: nan\n",
      "Iteration: 59000, Loss: nan\n",
      "Iteration: 60000, Loss: nan\n",
      "Iteration: 61000, Loss: nan\n",
      "Iteration: 62000, Loss: nan\n",
      "Iteration: 63000, Loss: nan\n",
      "Iteration: 64000, Loss: nan\n",
      "Iteration: 65000, Loss: nan\n",
      "Iteration: 0, Loss: 5.534e+03\n",
      "Iteration: 1000, Loss: 7.183e+01\n",
      "Iteration: 2000, Loss: 4.088e+01\n",
      "Iteration: 3000, Loss: 4.468e+01\n",
      "Iteration: 4000, Loss: 4.618e+01\n",
      "Iteration: 5000, Loss: 4.821e+01\n",
      "Iteration: 6000, Loss: 2.316e+01\n",
      "Iteration: 7000, Loss: 3.893e+01\n",
      "Iteration: 8000, Loss: 2.256e+01\n",
      "Iteration: 9000, Loss: 3.184e+01\n",
      "Iteration: 10000, Loss: 2.529e+01\n",
      "Iteration: 11000, Loss: 3.061e+01\n",
      "Iteration: 12000, Loss: 2.473e+01\n",
      "Iteration: 13000, Loss: 3.752e+01\n",
      "Iteration: 14000, Loss: 2.510e+01\n",
      "Iteration: 15000, Loss: 3.964e+01\n",
      "Iteration: 16000, Loss: 3.137e+01\n",
      "Iteration: 17000, Loss: 4.303e+01\n",
      "Iteration: 18000, Loss: 2.808e+01\n",
      "Iteration: 19000, Loss: 2.728e+01\n",
      "Iteration: 20000, Loss: 2.641e+01\n",
      "Iteration: 21000, Loss: 2.284e+01\n",
      "Iteration: 22000, Loss: 2.848e+01\n",
      "Iteration: 23000, Loss: 3.276e+01\n",
      "Iteration: 24000, Loss: 4.002e+01\n",
      "Iteration: 25000, Loss: 4.260e+01\n",
      "Iteration: 26000, Loss: 8.331e+01\n",
      "Iteration: 27000, Loss: 3.198e+01\n",
      "Iteration: 28000, Loss: 2.607e+01\n",
      "Iteration: 29000, Loss: 5.590e+01\n",
      "Iteration: 30000, Loss: 1.889e+01\n",
      "Iteration: 31000, Loss: 4.257e+01\n",
      "Iteration: 32000, Loss: 3.688e+01\n",
      "Iteration: 33000, Loss: 3.186e+01\n",
      "Iteration: 34000, Loss: 2.804e+01\n",
      "Iteration: 35000, Loss: 1.691e+01\n",
      "Iteration: 36000, Loss: 3.462e+01\n",
      "Iteration: 37000, Loss: 1.918e+01\n",
      "Iteration: 38000, Loss: 1.724e+01\n",
      "Iteration: 39000, Loss: 2.048e+01\n",
      "Iteration: 40000, Loss: 3.795e+01\n",
      "Iteration: 41000, Loss: 4.069e+01\n",
      "Iteration: 42000, Loss: 3.153e+01\n",
      "Iteration: 43000, Loss: 4.912e+01\n",
      "Iteration: 44000, Loss: 8.106e+01\n",
      "Iteration: 45000, Loss: 2.479e+01\n",
      "Iteration: 46000, Loss: 4.675e+01\n",
      "Iteration: 47000, Loss: 3.327e+01\n",
      "Iteration: 48000, Loss: 2.774e+01\n",
      "Iteration: 49000, Loss: 2.342e+01\n",
      "Iteration: 50000, Loss: 2.942e+01\n",
      "Iteration: 51000, Loss: 3.738e+01\n",
      "Iteration: 52000, Loss: 2.657e+01\n",
      "Iteration: 53000, Loss: 2.531e+01\n",
      "Iteration: 54000, Loss: 2.419e+01\n",
      "Iteration: 55000, Loss: 4.455e+01\n",
      "Iteration: 56000, Loss: 2.550e+01\n",
      "Iteration: 57000, Loss: 2.170e+01\n",
      "Iteration: 58000, Loss: 2.356e+01\n",
      "Iteration: 59000, Loss: 2.464e+01\n",
      "Iteration: 60000, Loss: 2.950e+01\n",
      "Iteration: 61000, Loss: 1.675e+01\n",
      "Iteration: 62000, Loss: 3.699e+01\n",
      "Iteration: 63000, Loss: 1.669e+01\n",
      "Iteration: 64000, Loss: 3.398e+01\n",
      "Iteration: 65000, Loss: 4.652e+01\n",
      "Iteration: 0, Loss: 5.346e+03\n",
      "Iteration: 1000, Loss: 4.774e+01\n",
      "Iteration: 2000, Loss: 4.526e+01\n",
      "Iteration: 3000, Loss: 4.950e+01\n",
      "Iteration: 4000, Loss: 3.297e+01\n",
      "Iteration: 5000, Loss: 3.300e+01\n",
      "Iteration: 6000, Loss: 1.157e+02\n",
      "Iteration: 7000, Loss: 2.638e+01\n",
      "Iteration: 8000, Loss: 1.999e+02\n",
      "Iteration: 9000, Loss: 1.886e+01\n",
      "Iteration: 10000, Loss: 1.833e+01\n",
      "Iteration: 11000, Loss: 2.486e+01\n",
      "Iteration: 12000, Loss: 2.061e+01\n",
      "Iteration: 13000, Loss: 3.223e+01\n",
      "Iteration: 14000, Loss: 2.515e+01\n",
      "Iteration: 15000, Loss: 2.611e+01\n",
      "Iteration: 16000, Loss: 4.366e+01\n",
      "Iteration: 17000, Loss: 2.784e+01\n",
      "Iteration: 18000, Loss: 2.912e+01\n",
      "Iteration: 19000, Loss: 2.382e+01\n",
      "Iteration: 20000, Loss: 3.115e+01\n",
      "Iteration: 21000, Loss: 4.159e+01\n",
      "Iteration: 22000, Loss: 1.875e+01\n",
      "Iteration: 23000, Loss: 1.885e+01\n",
      "Iteration: 24000, Loss: 4.652e+01\n",
      "Iteration: 25000, Loss: 1.692e+01\n",
      "Iteration: 26000, Loss: 3.912e+01\n",
      "Iteration: 27000, Loss: 3.001e+01\n",
      "Iteration: 28000, Loss: 2.604e+01\n",
      "Iteration: 29000, Loss: 3.251e+01\n",
      "Iteration: 30000, Loss: 2.623e+01\n",
      "Iteration: 31000, Loss: 3.320e+01\n",
      "Iteration: 32000, Loss: 3.435e+01\n",
      "Iteration: 33000, Loss: 2.782e+01\n",
      "Iteration: 34000, Loss: 2.515e+01\n",
      "Iteration: 35000, Loss: 2.119e+01\n",
      "Iteration: 36000, Loss: 3.017e+01\n",
      "Iteration: 37000, Loss: 2.426e+01\n",
      "Iteration: 38000, Loss: 2.036e+01\n",
      "Iteration: 39000, Loss: 2.513e+01\n",
      "Iteration: 40000, Loss: 2.862e+01\n",
      "Iteration: 41000, Loss: 4.765e+02\n",
      "Iteration: 42000, Loss: 1.049e+02\n",
      "Iteration: 43000, Loss: 2.583e+01\n",
      "Iteration: 44000, Loss: 3.188e+01\n",
      "Iteration: 45000, Loss: 6.538e+02\n",
      "Iteration: 46000, Loss: 2.130e+01\n",
      "Iteration: 47000, Loss: 3.430e+01\n",
      "Iteration: 48000, Loss: 2.187e+01\n",
      "Iteration: 49000, Loss: 2.070e+01\n",
      "Iteration: 50000, Loss: 3.797e+01\n",
      "Iteration: 51000, Loss: 2.978e+01\n",
      "Iteration: 52000, Loss: 2.860e+01\n",
      "Iteration: 53000, Loss: 3.552e+01\n",
      "Iteration: 54000, Loss: 2.796e+01\n",
      "Iteration: 55000, Loss: 1.595e+01\n",
      "Iteration: 56000, Loss: 1.394e+01\n",
      "Iteration: 57000, Loss: 2.546e+01\n",
      "Iteration: 58000, Loss: 2.581e+01\n",
      "Iteration: 59000, Loss: 6.841e+01\n",
      "Iteration: 60000, Loss: 4.005e+01\n",
      "Iteration: 61000, Loss: 2.571e+01\n",
      "Iteration: 62000, Loss: 2.111e+01\n",
      "Iteration: 63000, Loss: 3.011e+01\n",
      "Iteration: 64000, Loss: 2.534e+01\n",
      "Iteration: 65000, Loss: 2.379e+01\n",
      "Iteration: 0, Loss: 5.543e+03\n",
      "Iteration: 1000, Loss: 5.179e+01\n",
      "Iteration: 2000, Loss: 6.622e+01\n",
      "Iteration: 3000, Loss: 5.326e+01\n",
      "Iteration: 4000, Loss: 3.916e+01\n",
      "Iteration: 5000, Loss: 4.034e+01\n",
      "Iteration: 6000, Loss: 2.976e+01\n",
      "Iteration: 7000, Loss: 3.089e+01\n",
      "Iteration: 8000, Loss: 3.613e+01\n",
      "Iteration: 9000, Loss: 3.058e+01\n",
      "Iteration: 10000, Loss: 2.661e+01\n",
      "Iteration: 11000, Loss: 3.393e+01\n",
      "Iteration: 12000, Loss: 2.476e+01\n",
      "Iteration: 13000, Loss: 3.443e+01\n",
      "Iteration: 14000, Loss: 2.575e+01\n",
      "Iteration: 15000, Loss: 2.224e+01\n",
      "Iteration: 16000, Loss: 3.431e+01\n",
      "Iteration: 17000, Loss: 1.402e+03\n",
      "Iteration: 18000, Loss: 7.633e+01\n",
      "Iteration: 19000, Loss: 3.359e+01\n",
      "Iteration: 20000, Loss: 2.434e+01\n",
      "Iteration: 21000, Loss: 5.955e+01\n",
      "Iteration: 22000, Loss: 3.562e+01\n",
      "Iteration: 23000, Loss: 2.724e+01\n",
      "Iteration: 24000, Loss: 2.747e+01\n",
      "Iteration: 25000, Loss: 3.088e+01\n",
      "Iteration: 26000, Loss: 3.395e+01\n",
      "Iteration: 27000, Loss: 4.176e+01\n",
      "Iteration: 28000, Loss: 3.441e+01\n",
      "Iteration: 29000, Loss: 2.723e+01\n",
      "Iteration: 30000, Loss: 9.198e+01\n",
      "Iteration: 31000, Loss: 2.405e+03\n",
      "Iteration: 32000, Loss: 2.882e+01\n",
      "Iteration: 33000, Loss: 2.588e+01\n",
      "Iteration: 34000, Loss: 7.381e+01\n",
      "Iteration: 35000, Loss: 4.354e+02\n",
      "Iteration: 36000, Loss: 4.498e+01\n",
      "Iteration: 37000, Loss: 2.898e+01\n",
      "Iteration: 38000, Loss: 4.125e+01\n",
      "Iteration: 39000, Loss: 2.997e+01\n",
      "Iteration: 40000, Loss: 2.681e+01\n",
      "Iteration: 41000, Loss: 2.590e+01\n",
      "Iteration: 42000, Loss: 2.691e+01\n",
      "Iteration: 43000, Loss: 7.884e+01\n",
      "Iteration: 44000, Loss: 2.550e+01\n",
      "Iteration: 45000, Loss: 3.822e+01\n",
      "Iteration: 46000, Loss: 1.742e+01\n",
      "Iteration: 47000, Loss: 2.418e+01\n",
      "Iteration: 48000, Loss: 3.056e+01\n",
      "Iteration: 49000, Loss: 3.693e+01\n",
      "Iteration: 50000, Loss: 2.160e+01\n",
      "Iteration: 51000, Loss: 4.725e+01\n",
      "Iteration: 52000, Loss: 3.980e+01\n",
      "Iteration: 53000, Loss: 1.946e+01\n",
      "Iteration: 54000, Loss: 1.973e+01\n",
      "Iteration: 55000, Loss: 1.663e+01\n",
      "Iteration: 56000, Loss: 6.796e+01\n",
      "Iteration: 57000, Loss: 2.436e+01\n",
      "Iteration: 58000, Loss: 2.872e+01\n",
      "Iteration: 59000, Loss: 3.256e+01\n",
      "Iteration: 60000, Loss: 1.791e+01\n",
      "Iteration: 61000, Loss: 1.669e+01\n",
      "Iteration: 62000, Loss: 3.973e+01\n",
      "Iteration: 63000, Loss: 1.474e+01\n",
      "Iteration: 64000, Loss: 3.342e+01\n",
      "Iteration: 65000, Loss: 2.648e+01\n",
      "Iteration: 0, Loss: 5.540e+03\n",
      "Iteration: 1000, Loss: 4.789e+01\n",
      "Iteration: 2000, Loss: 5.682e+01\n",
      "Iteration: 3000, Loss: 6.052e+01\n",
      "Iteration: 4000, Loss: 4.017e+01\n",
      "Iteration: 5000, Loss: 2.594e+01\n",
      "Iteration: 6000, Loss: 3.223e+01\n",
      "Iteration: 7000, Loss: 3.883e+01\n",
      "Iteration: 8000, Loss: 2.872e+01\n",
      "Iteration: 9000, Loss: 3.656e+01\n",
      "Iteration: 10000, Loss: 3.413e+01\n",
      "Iteration: 11000, Loss: 3.797e+01\n",
      "Iteration: 12000, Loss: 4.223e+01\n",
      "Iteration: 13000, Loss: 2.396e+01\n",
      "Iteration: 14000, Loss: 3.883e+01\n",
      "Iteration: 15000, Loss: 2.773e+01\n",
      "Iteration: 16000, Loss: 3.169e+01\n",
      "Iteration: 17000, Loss: 4.316e+01\n",
      "Iteration: 18000, Loss: 3.143e+01\n",
      "Iteration: 19000, Loss: 2.483e+01\n",
      "Iteration: 20000, Loss: 3.242e+01\n",
      "Iteration: 21000, Loss: 3.033e+01\n",
      "Iteration: 22000, Loss: 2.853e+01\n",
      "Iteration: 23000, Loss: 2.466e+01\n",
      "Iteration: 24000, Loss: 3.219e+01\n",
      "Iteration: 25000, Loss: 2.140e+01\n",
      "Iteration: 26000, Loss: 2.893e+01\n",
      "Iteration: 27000, Loss: 1.970e+01\n",
      "Iteration: 28000, Loss: 4.216e+01\n",
      "Iteration: 29000, Loss: 2.232e+01\n",
      "Iteration: 30000, Loss: 4.139e+01\n",
      "Iteration: 31000, Loss: 4.699e+01\n",
      "Iteration: 32000, Loss: 2.424e+01\n",
      "Iteration: 33000, Loss: 4.344e+01\n",
      "Iteration: 34000, Loss: 4.387e+01\n",
      "Iteration: 35000, Loss: 3.554e+01\n",
      "Iteration: 36000, Loss: 3.705e+01\n",
      "Iteration: 37000, Loss: 3.387e+01\n",
      "Iteration: 38000, Loss: 2.568e+01\n",
      "Iteration: 39000, Loss: 5.767e+01\n",
      "Iteration: 40000, Loss: 3.708e+01\n",
      "Iteration: 41000, Loss: 2.121e+01\n",
      "Iteration: 42000, Loss: 2.944e+02\n",
      "Iteration: 43000, Loss: 4.587e+01\n",
      "Iteration: 44000, Loss: 3.247e+01\n",
      "Iteration: 45000, Loss: 2.337e+01\n",
      "Iteration: 46000, Loss: 2.423e+01\n",
      "Iteration: 47000, Loss: 2.202e+01\n",
      "Iteration: 48000, Loss: 7.696e+01\n",
      "Iteration: 49000, Loss: 2.988e+01\n",
      "Iteration: 50000, Loss: 2.049e+01\n",
      "Iteration: 51000, Loss: 2.073e+02\n",
      "Iteration: 52000, Loss: 2.371e+01\n",
      "Iteration: 53000, Loss: 6.973e+01\n",
      "Iteration: 54000, Loss: 1.979e+01\n",
      "Iteration: 55000, Loss: 9.338e+01\n",
      "Iteration: 56000, Loss: 3.116e+01\n",
      "Iteration: 57000, Loss: 7.310e+01\n",
      "Iteration: 58000, Loss: 2.027e+01\n",
      "Iteration: 59000, Loss: 2.610e+01\n",
      "Iteration: 60000, Loss: 2.298e+01\n",
      "Iteration: 61000, Loss: 2.243e+01\n",
      "Iteration: 62000, Loss: 3.914e+01\n",
      "Iteration: 63000, Loss: 2.136e+01\n",
      "Iteration: 64000, Loss: 2.097e+01\n",
      "Iteration: 65000, Loss: 2.410e+01\n",
      "Iteration: 0, Loss: 5.297e+03\n",
      "Iteration: 1000, Loss: 4.592e+01\n",
      "Iteration: 2000, Loss: 4.185e+01\n",
      "Iteration: 3000, Loss: 3.625e+01\n",
      "Iteration: 4000, Loss: 6.015e+01\n",
      "Iteration: 5000, Loss: 5.160e+01\n",
      "Iteration: 6000, Loss: 3.100e+01\n",
      "Iteration: 7000, Loss: 3.052e+01\n",
      "Iteration: 8000, Loss: 2.720e+01\n",
      "Iteration: 9000, Loss: 9.905e+01\n",
      "Iteration: 10000, Loss: 3.389e+01\n",
      "Iteration: 11000, Loss: 2.785e+01\n",
      "Iteration: 12000, Loss: 4.523e+01\n",
      "Iteration: 13000, Loss: 2.855e+01\n",
      "Iteration: 14000, Loss: 3.189e+01\n",
      "Iteration: 15000, Loss: 2.329e+01\n",
      "Iteration: 16000, Loss: 3.337e+01\n",
      "Iteration: 17000, Loss: 4.285e+01\n",
      "Iteration: 18000, Loss: 3.180e+01\n",
      "Iteration: 19000, Loss: 2.304e+01\n",
      "Iteration: 20000, Loss: 2.469e+01\n",
      "Iteration: 21000, Loss: 5.770e+01\n",
      "Iteration: 22000, Loss: 3.228e+01\n",
      "Iteration: 23000, Loss: 1.596e+01\n",
      "Iteration: 24000, Loss: 2.422e+01\n",
      "Iteration: 25000, Loss: 2.124e+01\n",
      "Iteration: 26000, Loss: 2.417e+01\n",
      "Iteration: 27000, Loss: 2.400e+01\n",
      "Iteration: 28000, Loss: 2.277e+01\n",
      "Iteration: 29000, Loss: 2.524e+01\n",
      "Iteration: 30000, Loss: 3.423e+01\n",
      "Iteration: 31000, Loss: 3.028e+01\n",
      "Iteration: 32000, Loss: 3.520e+01\n",
      "Iteration: 33000, Loss: 2.556e+01\n",
      "Iteration: 34000, Loss: 2.861e+01\n",
      "Iteration: 35000, Loss: 3.647e+01\n",
      "Iteration: 36000, Loss: 2.508e+03\n",
      "Iteration: 37000, Loss: 1.549e+02\n",
      "Iteration: 38000, Loss: 2.591e+01\n",
      "Iteration: 39000, Loss: 2.625e+01\n",
      "Iteration: 40000, Loss: 2.284e+01\n",
      "Iteration: 41000, Loss: 1.874e+01\n",
      "Iteration: 42000, Loss: 2.896e+01\n",
      "Iteration: 43000, Loss: 2.629e+01\n",
      "Iteration: 44000, Loss: 3.171e+01\n",
      "Iteration: 45000, Loss: 1.942e+01\n",
      "Iteration: 46000, Loss: 2.398e+01\n",
      "Iteration: 47000, Loss: 3.179e+01\n",
      "Iteration: 48000, Loss: 3.995e+01\n",
      "Iteration: 49000, Loss: 3.729e+01\n",
      "Iteration: 50000, Loss: 6.334e+01\n",
      "Iteration: 51000, Loss: 2.962e+01\n",
      "Iteration: 52000, Loss: 4.336e+01\n",
      "Iteration: 53000, Loss: 2.045e+01\n",
      "Iteration: 54000, Loss: 1.609e+01\n",
      "Iteration: 55000, Loss: 3.041e+01\n",
      "Iteration: 56000, Loss: 2.198e+01\n",
      "Iteration: 57000, Loss: 2.759e+01\n",
      "Iteration: 58000, Loss: 2.727e+01\n",
      "Iteration: 59000, Loss: 2.096e+01\n",
      "Iteration: 60000, Loss: 3.241e+01\n",
      "Iteration: 61000, Loss: 3.246e+01\n",
      "Iteration: 62000, Loss: 3.189e+01\n",
      "Iteration: 63000, Loss: 2.916e+01\n",
      "Iteration: 64000, Loss: 1.917e+01\n",
      "Iteration: 65000, Loss: 2.744e+01\n",
      "Iteration: 0, Loss: 5.389e+03\n",
      "Iteration: 1000, Loss: 6.831e+01\n",
      "Iteration: 2000, Loss: 4.093e+01\n",
      "Iteration: 3000, Loss: 5.644e+01\n",
      "Iteration: 4000, Loss: 4.008e+01\n",
      "Iteration: 5000, Loss: 2.778e+01\n",
      "Iteration: 6000, Loss: 3.256e+01\n",
      "Iteration: 7000, Loss: 3.957e+01\n",
      "Iteration: 8000, Loss: 4.746e+01\n",
      "Iteration: 9000, Loss: 2.304e+01\n",
      "Iteration: 10000, Loss: 2.937e+01\n",
      "Iteration: 11000, Loss: 5.315e+01\n",
      "Iteration: 12000, Loss: 3.378e+01\n",
      "Iteration: 13000, Loss: 3.033e+01\n",
      "Iteration: 14000, Loss: 2.510e+01\n",
      "Iteration: 15000, Loss: 2.695e+01\n",
      "Iteration: 16000, Loss: 4.553e+01\n",
      "Iteration: 17000, Loss: 1.980e+01\n",
      "Iteration: 18000, Loss: 3.429e+01\n",
      "Iteration: 19000, Loss: 3.540e+01\n",
      "Iteration: 20000, Loss: 4.355e+01\n",
      "Iteration: 21000, Loss: 3.279e+01\n",
      "Iteration: 22000, Loss: 2.257e+01\n",
      "Iteration: 23000, Loss: 1.840e+01\n",
      "Iteration: 24000, Loss: 4.117e+01\n",
      "Iteration: 25000, Loss: 2.149e+01\n",
      "Iteration: 26000, Loss: 1.354e+02\n",
      "Iteration: 27000, Loss: 2.717e+01\n",
      "Iteration: 28000, Loss: 2.748e+01\n",
      "Iteration: 29000, Loss: 3.442e+01\n",
      "Iteration: 30000, Loss: 3.604e+01\n",
      "Iteration: 31000, Loss: 2.737e+01\n",
      "Iteration: 32000, Loss: 2.757e+01\n",
      "Iteration: 33000, Loss: 2.803e+01\n",
      "Iteration: 34000, Loss: 2.538e+01\n",
      "Iteration: 35000, Loss: 2.012e+01\n",
      "Iteration: 36000, Loss: 2.003e+01\n",
      "Iteration: 37000, Loss: 1.937e+01\n",
      "Iteration: 38000, Loss: 2.040e+01\n",
      "Iteration: 39000, Loss: 1.799e+01\n",
      "Iteration: 40000, Loss: 2.122e+01\n",
      "Iteration: 41000, Loss: 3.341e+01\n",
      "Iteration: 42000, Loss: 2.214e+02\n",
      "Iteration: 43000, Loss: 4.748e+01\n",
      "Iteration: 44000, Loss: 1.744e+01\n",
      "Iteration: 45000, Loss: 2.506e+01\n",
      "Iteration: 46000, Loss: 1.884e+01\n",
      "Iteration: 47000, Loss: 1.624e+01\n",
      "Iteration: 48000, Loss: 2.494e+01\n",
      "Iteration: 49000, Loss: 4.009e+01\n",
      "Iteration: 50000, Loss: 3.263e+01\n",
      "Iteration: 51000, Loss: 2.975e+01\n",
      "Iteration: 52000, Loss: 4.424e+01\n",
      "Iteration: 53000, Loss: 1.861e+01\n",
      "Iteration: 54000, Loss: 2.806e+01\n",
      "Iteration: 55000, Loss: 1.815e+01\n",
      "Iteration: 56000, Loss: 6.021e+01\n",
      "Iteration: 57000, Loss: 1.416e+01\n",
      "Iteration: 58000, Loss: 2.516e+01\n",
      "Iteration: 59000, Loss: 2.723e+01\n",
      "Iteration: 60000, Loss: 5.535e+01\n",
      "Iteration: 61000, Loss: 2.052e+01\n",
      "Iteration: 62000, Loss: 2.670e+01\n",
      "Iteration: 63000, Loss: 1.612e+01\n",
      "Iteration: 64000, Loss: 9.163e+01\n",
      "Iteration: 65000, Loss: 3.581e+01\n",
      "Iteration: 0, Loss: 5.494e+03\n",
      "Iteration: 1000, Loss: 5.218e+01\n",
      "Iteration: 2000, Loss: 6.888e+01\n",
      "Iteration: 3000, Loss: 4.666e+01\n",
      "Iteration: 4000, Loss: 6.246e+01\n",
      "Iteration: 5000, Loss: 3.840e+01\n",
      "Iteration: 6000, Loss: 5.400e+01\n",
      "Iteration: 7000, Loss: 3.358e+01\n",
      "Iteration: 8000, Loss: 4.463e+01\n",
      "Iteration: 9000, Loss: 4.297e+01\n",
      "Iteration: 10000, Loss: 5.713e+01\n",
      "Iteration: 11000, Loss: 4.036e+01\n",
      "Iteration: 12000, Loss: 3.658e+01\n",
      "Iteration: 13000, Loss: 2.685e+01\n",
      "Iteration: 14000, Loss: 3.060e+01\n",
      "Iteration: 15000, Loss: 3.067e+01\n",
      "Iteration: 16000, Loss: 4.584e+01\n",
      "Iteration: 17000, Loss: 2.595e+01\n",
      "Iteration: 18000, Loss: 2.957e+01\n",
      "Iteration: 19000, Loss: 2.103e+01\n",
      "Iteration: 20000, Loss: 2.892e+01\n",
      "Iteration: 21000, Loss: 2.773e+01\n",
      "Iteration: 22000, Loss: 1.977e+01\n",
      "Iteration: 23000, Loss: 1.590e+01\n",
      "Iteration: 24000, Loss: 3.416e+01\n",
      "Iteration: 25000, Loss: 3.257e+01\n",
      "Iteration: 26000, Loss: 2.663e+01\n",
      "Iteration: 27000, Loss: 3.673e+01\n",
      "Iteration: 28000, Loss: 1.277e+01\n",
      "Iteration: 29000, Loss: 3.344e+01\n",
      "Iteration: 30000, Loss: 4.008e+01\n",
      "Iteration: 31000, Loss: 2.500e+01\n",
      "Iteration: 32000, Loss: 5.967e+01\n",
      "Iteration: 33000, Loss: 3.458e+01\n",
      "Iteration: 34000, Loss: 4.393e+01\n",
      "Iteration: 35000, Loss: 3.632e+01\n",
      "Iteration: 36000, Loss: 3.111e+01\n",
      "Iteration: 37000, Loss: 2.239e+01\n",
      "Iteration: 38000, Loss: 2.270e+01\n",
      "Iteration: 39000, Loss: 3.823e+01\n",
      "Iteration: 40000, Loss: 2.441e+01\n",
      "Iteration: 41000, Loss: 1.960e+01\n",
      "Iteration: 42000, Loss: 1.843e+01\n",
      "Iteration: 43000, Loss: 2.407e+02\n",
      "Iteration: 44000, Loss: 9.556e+01\n",
      "Iteration: 45000, Loss: 1.141e+02\n",
      "Iteration: 46000, Loss: 5.456e+01\n",
      "Iteration: 47000, Loss: 4.487e+01\n",
      "Iteration: 48000, Loss: 2.570e+01\n",
      "Iteration: 49000, Loss: 2.828e+01\n",
      "Iteration: 50000, Loss: 1.753e+01\n",
      "Iteration: 51000, Loss: 2.260e+01\n",
      "Iteration: 52000, Loss: 3.496e+01\n",
      "Iteration: 53000, Loss: 1.850e+01\n",
      "Iteration: 54000, Loss: 2.294e+01\n",
      "Iteration: 55000, Loss: 2.748e+01\n",
      "Iteration: 56000, Loss: 4.001e+01\n",
      "Iteration: 57000, Loss: 3.019e+01\n",
      "Iteration: 58000, Loss: 5.007e+01\n",
      "Iteration: 59000, Loss: 2.634e+01\n",
      "Iteration: 60000, Loss: 2.586e+01\n",
      "Iteration: 61000, Loss: 2.865e+01\n",
      "Iteration: 62000, Loss: 3.126e+01\n",
      "Iteration: 63000, Loss: 2.672e+01\n",
      "Iteration: 64000, Loss: 2.385e+01\n",
      "Iteration: 65000, Loss: 2.819e+01\n",
      "Iteration: 0, Loss: 4.551e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m loss_boundary \u001b[38;5;241m=\u001b[39m (loss_bc(fbsde, u, boundary_xs) \u001b[38;5;241m+\u001b[39m loss_diff(fbsde, u, boundary_ts, boundary_xs))\u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_interior \u001b[38;5;241m+\u001b[39m loss_boundary\n\u001b[0;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m pinn_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m run_boundary_losses\u001b[38;5;241m.\u001b[39mappend(loss_boundary\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/presentation/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/presentation/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for activation in [torch.sin, F.relu]:\n",
    "for activation in [torch.sin]:\n",
    "    activation_str = None\n",
    "    if activation == torch.sin:\n",
    "        activation_str = \"sin\"\n",
    "    elif activation == F.relu:\n",
    "        activation_str = \"relu\"\n",
    "    for d in [100, 3]:\n",
    "        losses = []\n",
    "        boundary_losses = []\n",
    "        times = []\n",
    "        zeta = torch.tensor(int(d / 2) * [1., 0.5] + (d % 2) * [1.], device=device)\n",
    "        for run in range(runs):\n",
    "            run_losses = []\n",
    "            run_boundary_losses = []\n",
    "            run_times = []\n",
    "            pinn_network = NAIS_Net_Untied(d+1, 256, 4, 1, activation, epsilon, h).to(device)\n",
    "            pinn_optimizer = torch.optim.LBFGS(pinn_network.parameters(), lr=learning_rate) if optimizer == \"LBFGS\" else torch.optim.Adam(pinn_network.parameters(), lr=learning_rate)\n",
    "            for iteration in range(num_iterations):\n",
    "                start_time = time.time()\n",
    "                interior_ts = torch.rand((batch_size, 1), requires_grad=True, device=device)\n",
    "                # This is where the markdown remark comes in\n",
    "                interior_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * interior_ts + fbsde.volatility * torch.sqrt(interior_ts) * torch.randn((batch_size, d), device=device))\n",
    "                #interior_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                interior_xs.requires_grad_(True)\n",
    "\n",
    "                boundary_ts = fbsde.T * torch.ones((batch_size, 1), device=device)\n",
    "                boundary_ts.requires_grad_(True)\n",
    "                #boundary_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                boundary_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * boundary_ts + fbsde.volatility * torch.sqrt(boundary_ts) * torch.randn((batch_size, d), device=device))\n",
    "                boundary_xs.requires_grad_(True)\n",
    "\n",
    "                if optimizer == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        pinn_optimizer.zero_grad()\n",
    "                        xs_i = interior_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_i = interior_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_i = torch.cat((ts_i, xs_i), dim=-1)\n",
    "                        u_i = pinn_network(sample_i)\n",
    "                        loss_interior = loss_diff(fbsde, u_i, ts_i, xs_i)/batch_size\n",
    "                        xs_b = boundary_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_b = boundary_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_b = torch.cat((ts_b, xs_b), dim=-1)\n",
    "                        u_b = pinn_network(sample_b)\n",
    "                        loss_boundary = (loss_bc(fbsde, u_b, xs_b) + loss_diff(fbsde, u_b, ts_b, xs_b))/batch_size\n",
    "                        loss = loss_interior + loss_boundary\n",
    "                        loss.backward()\n",
    "                        print(loss)\n",
    "                        return loss\n",
    "                    loss = pinn_optimizer.step(closure)\n",
    "                elif optimizer == \"Adam\":\n",
    "                    pinn_optimizer.zero_grad()\n",
    "                    interior_sample = torch.cat((interior_ts, interior_xs), dim=-1)\n",
    "                    u = pinn_network(interior_sample)\n",
    "                    loss_interior = loss_diff(fbsde, u, interior_ts, interior_xs)/batch_size\n",
    "                    boundary_sample = torch.cat((boundary_ts, boundary_xs), dim=-1)\n",
    "                    u = pinn_network(boundary_sample)\n",
    "                    loss_boundary = (loss_bc(fbsde, u, boundary_xs) + loss_diff(fbsde, u, boundary_ts, boundary_xs))/batch_size\n",
    "                    loss = loss_interior + loss_boundary\n",
    "                    loss.backward()\n",
    "                    pinn_optimizer.step()\n",
    "                    run_boundary_losses.append(loss_boundary.item())\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Iteration: %d, Loss: %.3e\" % (iteration, loss.item()))\n",
    "                    torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_iteration_%d.pt\" % (activation_str, d, run, iteration))\n",
    "                run_losses.append(loss.item())\n",
    "                run_times.append(time.time() - start_time)\n",
    "            losses.append(run_losses)\n",
    "            boundary_losses.append(run_boundary_losses)\n",
    "            times.append(run_times)\n",
    "            torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_trained.pt\" % (activation_str, d, run))\n",
    "        with open(\"PINN_%s/losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(losses, f)\n",
    "        with open(\"PINN_%s/terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(boundary_losses, f)\n",
    "        with open(\"PINN_%s/times_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(run_times, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100\n",
    "f = open(\"PINN_%s/01losses_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "losses_old = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"PINN_%s/01terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "boundary_losses_old = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"PINN_%s/01times_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "times_old = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"PINN_%s/losses_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "new_losses = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"PINN_%s/terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "new_boundary_losses = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"PINN_%s/times_dimensions_%d.pkl\" % (activation_str, d), \"rb\")\n",
    "new_times = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_losses = losses_old + new_losses\n",
    "new_boundary_losses = boundary_losses_old + new_boundary_losses\n",
    "new_times = times_old + new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_str = \"sin\"\n",
    "with open(\"PINN_%s/new_losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "    pickle.dump(new_losses, f)\n",
    "with open(\"PINN_%s/new_terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "    pickle.dump(new_boundary_losses, f)\n",
    "with open(\"PINN_%s/new_times_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "    pickle.dump(new_times, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74957"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:presentation]",
   "language": "python",
   "name": "conda-env-presentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
