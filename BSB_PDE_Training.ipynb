{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "from networks import *\n",
    "from fbsde import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h = 1\n",
    "epsilon = 0.01\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_per_dim = 5\n",
    "batch_size = 100\n",
    "num_iterations = 66000\n",
    "mse = nn.MSELoss(reduction=\"sum\")\n",
    "runs = 10\n",
    "r = 0.05\n",
    "volatility = 0.4\n",
    "T = 1\n",
    "fbsde = BS_Barenblatt(volatility, r, 1)\n",
    "#optimizer = \"LBFGS\" # Can use LBFGS or Adam\n",
    "#learning_rate = 1\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_diff(pde, u, t, x):\n",
    "  u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "  Du = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "  #Hessian H[i][j] is derivative with respect to jth variable then with respect to ith variable or is it the other way around\n",
    "  I_N = torch.eye(x.shape[-1], device=device)\n",
    "  def get_vjp(v):\n",
    "    return torch.autograd.grad(Du, x, grad_outputs=v.repeat(x.shape[0], 1), create_graph=True)\n",
    "  D2u = torch.vmap(get_vjp)(I_N)[0]\n",
    "  if len(x.shape) > 1:\n",
    "    D2u = D2u.swapaxes(0, 1)\n",
    "  A = D2u @ pde.sigma(t,x,u) @ pde.sigma(t, x, u).transpose(-2, -1)\n",
    "  trace = torch.diagonal(A, dim1=-2, dim2=-1).sum(dim=-1, keepdim=True)\n",
    "  #trace = torch.vmap(torch.trace)(A)\n",
    "  # in the code D2u[sample][i][j] is the derivative with respect to ith variable then jth variable\n",
    "  f = pde.phi(t, x, u, Du) - torch.sum(Du * pde.mu(t, x, u, Du), dim=-1, keepdim=True) - 1/2 * trace\n",
    "  return mse(u_t, f)\n",
    "\n",
    "def loss_bc(pde, u, x):\n",
    "  return mse(pde.g(x), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of interior training points is specific to the Black-Scholes-Barenblatt PDE as there is a simple closed form solution for the forward process in the associated PDE and so we can train on points chosen in a similar way to the FBSDE loss case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 5.548e+03\n",
      "Iteration: 1000, Loss: 5.736e+01\n",
      "Iteration: 2000, Loss: 4.259e+01\n",
      "Iteration: 3000, Loss: 3.520e+01\n",
      "Iteration: 4000, Loss: 3.900e+01\n",
      "Iteration: 5000, Loss: 5.461e+01\n",
      "Iteration: 6000, Loss: 2.995e+01\n",
      "Iteration: 7000, Loss: 6.977e+01\n",
      "Iteration: 8000, Loss: 2.105e+01\n",
      "Iteration: 9000, Loss: 2.933e+01\n",
      "Iteration: 10000, Loss: 2.945e+01\n",
      "Iteration: 11000, Loss: 3.522e+01\n",
      "Iteration: 12000, Loss: 2.073e+01\n",
      "Iteration: 13000, Loss: 2.818e+01\n",
      "Iteration: 14000, Loss: 1.924e+01\n",
      "Iteration: 15000, Loss: 2.404e+01\n",
      "Iteration: 16000, Loss: 4.114e+01\n",
      "Iteration: 17000, Loss: 2.426e+01\n",
      "Iteration: 18000, Loss: 2.158e+01\n",
      "Iteration: 19000, Loss: 3.909e+01\n",
      "Iteration: 20000, Loss: 5.680e+01\n",
      "Iteration: 21000, Loss: 2.824e+01\n",
      "Iteration: 22000, Loss: 3.592e+01\n",
      "Iteration: 23000, Loss: 2.802e+01\n",
      "Iteration: 24000, Loss: 1.922e+01\n",
      "Iteration: 25000, Loss: 1.850e+01\n",
      "Iteration: 26000, Loss: 4.137e+01\n",
      "Iteration: 27000, Loss: 2.572e+01\n",
      "Iteration: 28000, Loss: 4.837e+01\n",
      "Iteration: 29000, Loss: 2.268e+01\n",
      "Iteration: 30000, Loss: 2.254e+01\n",
      "Iteration: 31000, Loss: 3.606e+01\n",
      "Iteration: 32000, Loss: 3.243e+01\n",
      "Iteration: 33000, Loss: 2.769e+01\n",
      "Iteration: 34000, Loss: 3.085e+01\n",
      "Iteration: 35000, Loss: 3.365e+01\n",
      "Iteration: 36000, Loss: 2.412e+01\n",
      "Iteration: 37000, Loss: 3.479e+01\n",
      "Iteration: 38000, Loss: 3.481e+01\n",
      "Iteration: 39000, Loss: 2.915e+01\n",
      "Iteration: 40000, Loss: 7.772e+01\n",
      "Iteration: 41000, Loss: 1.893e+01\n",
      "Iteration: 42000, Loss: 2.434e+01\n",
      "Iteration: 43000, Loss: 1.826e+01\n",
      "Iteration: 44000, Loss: 1.797e+01\n",
      "Iteration: 45000, Loss: 4.702e+01\n",
      "Iteration: 46000, Loss: 2.203e+01\n",
      "Iteration: 47000, Loss: 3.851e+01\n",
      "Iteration: 48000, Loss: 3.543e+01\n",
      "Iteration: 49000, Loss: 2.434e+01\n",
      "Iteration: 50000, Loss: 2.212e+01\n",
      "Iteration: 51000, Loss: 2.649e+01\n",
      "Iteration: 52000, Loss: 2.810e+01\n",
      "Iteration: 53000, Loss: 2.736e+01\n",
      "Iteration: 54000, Loss: 2.591e+01\n",
      "Iteration: 55000, Loss: 2.361e+01\n",
      "Iteration: 56000, Loss: 2.814e+01\n",
      "Iteration: 57000, Loss: 3.102e+01\n",
      "Iteration: 58000, Loss: 4.444e+01\n",
      "Iteration: 59000, Loss: 3.169e+01\n",
      "Iteration: 60000, Loss: 2.282e+01\n",
      "Iteration: 61000, Loss: 1.625e+01\n",
      "Iteration: 62000, Loss: 1.562e+01\n",
      "Iteration: 63000, Loss: 2.594e+01\n",
      "Iteration: 64000, Loss: 2.370e+01\n",
      "Iteration: 65000, Loss: 2.880e+01\n",
      "Iteration: 0, Loss: 5.686e+03\n",
      "Iteration: 1000, Loss: 5.021e+01\n",
      "Iteration: 2000, Loss: 5.738e+01\n",
      "Iteration: 3000, Loss: 3.590e+01\n",
      "Iteration: 4000, Loss: 1.950e+01\n",
      "Iteration: 5000, Loss: 3.274e+01\n",
      "Iteration: 6000, Loss: 3.417e+01\n",
      "Iteration: 7000, Loss: 2.717e+01\n",
      "Iteration: 8000, Loss: 1.789e+01\n",
      "Iteration: 9000, Loss: 4.086e+01\n",
      "Iteration: 10000, Loss: 2.952e+01\n",
      "Iteration: 11000, Loss: 2.825e+01\n",
      "Iteration: 12000, Loss: 2.561e+01\n",
      "Iteration: 13000, Loss: 1.184e+02\n",
      "Iteration: 14000, Loss: 2.876e+01\n",
      "Iteration: 15000, Loss: 2.137e+01\n",
      "Iteration: 16000, Loss: 2.284e+01\n",
      "Iteration: 17000, Loss: 2.878e+01\n",
      "Iteration: 18000, Loss: 2.286e+01\n",
      "Iteration: 19000, Loss: 8.074e+01\n",
      "Iteration: 20000, Loss: 3.043e+01\n",
      "Iteration: 21000, Loss: 2.168e+01\n",
      "Iteration: 22000, Loss: 2.526e+01\n",
      "Iteration: 23000, Loss: 2.867e+01\n",
      "Iteration: 24000, Loss: 3.794e+01\n",
      "Iteration: 25000, Loss: 2.718e+01\n",
      "Iteration: 26000, Loss: 2.838e+01\n",
      "Iteration: 27000, Loss: 3.956e+01\n",
      "Iteration: 28000, Loss: 2.104e+01\n",
      "Iteration: 29000, Loss: 2.927e+01\n",
      "Iteration: 30000, Loss: 3.601e+01\n",
      "Iteration: 31000, Loss: 2.149e+01\n",
      "Iteration: 32000, Loss: 2.875e+01\n",
      "Iteration: 33000, Loss: 2.788e+01\n",
      "Iteration: 34000, Loss: 2.565e+01\n",
      "Iteration: 35000, Loss: 2.759e+01\n",
      "Iteration: 36000, Loss: 3.225e+01\n",
      "Iteration: 37000, Loss: 1.753e+01\n",
      "Iteration: 38000, Loss: 4.580e+01\n",
      "Iteration: 39000, Loss: 2.766e+01\n",
      "Iteration: 40000, Loss: 3.658e+01\n",
      "Iteration: 41000, Loss: 1.986e+01\n",
      "Iteration: 42000, Loss: 1.969e+01\n",
      "Iteration: 43000, Loss: 2.580e+01\n",
      "Iteration: 44000, Loss: 2.359e+01\n",
      "Iteration: 45000, Loss: 1.584e+01\n",
      "Iteration: 46000, Loss: 2.905e+01\n",
      "Iteration: 47000, Loss: 3.626e+01\n",
      "Iteration: 48000, Loss: 3.865e+01\n",
      "Iteration: 49000, Loss: 3.556e+01\n",
      "Iteration: 50000, Loss: 2.202e+01\n",
      "Iteration: 51000, Loss: 2.468e+01\n",
      "Iteration: 52000, Loss: 3.132e+01\n",
      "Iteration: 53000, Loss: 1.475e+01\n",
      "Iteration: 54000, Loss: 4.913e+01\n",
      "Iteration: 55000, Loss: 1.912e+01\n",
      "Iteration: 56000, Loss: 1.577e+01\n",
      "Iteration: 57000, Loss: 2.584e+01\n",
      "Iteration: 58000, Loss: 1.975e+01\n",
      "Iteration: 59000, Loss: 6.714e+01\n",
      "Iteration: 60000, Loss: 2.482e+01\n",
      "Iteration: 61000, Loss: 4.553e+01\n",
      "Iteration: 62000, Loss: 1.946e+01\n",
      "Iteration: 63000, Loss: 3.184e+01\n",
      "Iteration: 64000, Loss: 3.898e+01\n",
      "Iteration: 65000, Loss: 3.091e+01\n",
      "Iteration: 0, Loss: 5.637e+03\n",
      "Iteration: 1000, Loss: 4.822e+01\n",
      "Iteration: 2000, Loss: 3.417e+01\n",
      "Iteration: 3000, Loss: 4.998e+01\n",
      "Iteration: 4000, Loss: 4.418e+01\n",
      "Iteration: 5000, Loss: 3.684e+01\n",
      "Iteration: 6000, Loss: 3.471e+01\n",
      "Iteration: 7000, Loss: 3.129e+01\n",
      "Iteration: 8000, Loss: 9.148e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m loss_interior \u001b[38;5;241m=\u001b[39m loss_diff(fbsde, u, interior_ts, interior_xs)\u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     56\u001b[0m boundary_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((boundary_ts, boundary_xs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboundary_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m loss_boundary \u001b[38;5;241m=\u001b[39m (loss_bc(fbsde, u, boundary_xs) \u001b[38;5;241m+\u001b[39m loss_diff(fbsde, u, boundary_ts, boundary_xs))\u001b[38;5;241m/\u001b[39mbatch_size\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_interior \u001b[38;5;241m+\u001b[39m loss_boundary\n",
      "File \u001b[0;32m~/anaconda3/envs/presentation/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/networks.py:42\u001b[0m, in \u001b[0;36mNAIS_Net_Untied.forward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(u))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_state)):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_forward(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_state[i]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers_input[i](u))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for activation in [torch.sin, F.relu]:\n",
    "for activation in [torch.sin]:\n",
    "    activation_str = None\n",
    "    if activation == torch.sin:\n",
    "        activation_str = \"sin\"\n",
    "    elif activation == F.relu:\n",
    "        activation_str = \"relu\"\n",
    "    for d in [100, 3]:\n",
    "        losses = []\n",
    "        boundary_losses = []\n",
    "        times = []\n",
    "        zeta = torch.tensor(int(d / 2) * [1., 0.5] + (d % 2) * [1.], device=device)\n",
    "        for run in range(runs):\n",
    "            run_losses = []\n",
    "            run_boundary_losses = []\n",
    "            run_times = []\n",
    "            pinn_network = NAIS_Net_Untied(d+1, 256, 4, 1, activation, epsilon, h).to(device)\n",
    "            pinn_optimizer = torch.optim.LBFGS(pinn_network.parameters(), lr=learning_rate) if optimizer == \"LBFGS\" else torch.optim.Adam(pinn_network.parameters(), lr=learning_rate)\n",
    "            for iteration in range(num_iterations):\n",
    "                start_time = time.time()\n",
    "                interior_ts = torch.rand((batch_size, 1), requires_grad=True, device=device)\n",
    "                # This is where the markdown remark comes in\n",
    "                interior_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * interior_ts + fbsde.volatility * torch.sqrt(interior_ts) * torch.randn((batch_size, d), device=device))\n",
    "                #interior_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                interior_xs.requires_grad_(True)\n",
    "\n",
    "                boundary_ts = fbsde.T * torch.ones((batch_size, 1), device=device)\n",
    "                boundary_ts.requires_grad_(True)\n",
    "                #boundary_xs = sup_per_dim * torch.rand((batch_size, d), device=device)\n",
    "                boundary_xs = zeta.detach().clone().repeat(batch_size, 1) * torch.exp(-((fbsde.volatility)**2 / 2) * boundary_ts + fbsde.volatility * torch.sqrt(boundary_ts) * torch.randn((batch_size, d), device=device))\n",
    "                boundary_xs.requires_grad_(True)\n",
    "\n",
    "                if optimizer == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        pinn_optimizer.zero_grad()\n",
    "                        xs_i = interior_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_i = interior_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_i = torch.cat((ts_i, xs_i), dim=-1)\n",
    "                        u_i = pinn_network(sample_i)\n",
    "                        loss_interior = loss_diff(fbsde, u_i, ts_i, xs_i)/batch_size\n",
    "                        xs_b = boundary_xs.detach().clone().requires_grad_(True)\n",
    "                        ts_b = boundary_ts.detach().clone().requires_grad_(True)\n",
    "                        sample_b = torch.cat((ts_b, xs_b), dim=-1)\n",
    "                        u_b = pinn_network(sample_b)\n",
    "                        loss_boundary = loss_bc(fbsde, u_b, xs_b)/batch_size\n",
    "                        loss = loss_interior + loss_boundary\n",
    "                        loss.backward()\n",
    "                        print(loss)\n",
    "                        return loss\n",
    "                    loss = pinn_optimizer.step(closure)\n",
    "                elif optimizer == \"Adam\":\n",
    "                    pinn_optimizer.zero_grad()\n",
    "                    interior_sample = torch.cat((interior_ts, interior_xs), dim=-1)\n",
    "                    u = pinn_network(interior_sample)\n",
    "                    loss_interior = loss_diff(fbsde, u, interior_ts, interior_xs)/batch_size\n",
    "                    boundary_sample = torch.cat((boundary_ts, boundary_xs), dim=-1)\n",
    "                    u = pinn_network(boundary_sample)\n",
    "                    loss_boundary = loss_bc(fbsde, u, boundary_xs)/batch_size\n",
    "                    loss = loss_interior + loss_boundary\n",
    "                    loss.backward()\n",
    "                    pinn_optimizer.step()\n",
    "                    run_boundary_losses.append(loss_boundary.item())\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Iteration: %d, Loss: %.3e\" % (iteration, loss.item()))\n",
    "                    torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_iteration_%d.pt\" % (activation_str, d, run, iteration))\n",
    "                run_losses.append(loss.item())\n",
    "                run_times.append(time.time() - start_time)\n",
    "            losses.append(run_losses)\n",
    "            boundary_losses.append(run_boundary_losses)\n",
    "            times.append(run_times)\n",
    "            torch.save(pinn_network.state_dict(), \"PINN_%s/pde_pinn_dimensions_%d_run_%d_trained.pt\" % (activation_str, d, run))\n",
    "        with open(\"PINN_%s/losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(losses, f)\n",
    "        with open(\"PINN_%s/terminal_losses_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(boundary_losses, f)\n",
    "        with open(\"PINN_%s/times_dimensions_%d.pkl\" % (activation_str, d), \"wb\") as f:\n",
    "            pickle.dump(run_times, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:presentation]",
   "language": "python",
   "name": "conda-env-presentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
